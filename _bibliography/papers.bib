---
---

@misc{bakhitov2020frequentist,
  title={Frequentist Shrinkage under Inequality Constraints},
  author={Edvard Bakhitov},
  note={working paper,},
  abstract={This paper shows how to shrink extremum estimators towards inequality constraints motivated by economic theory. We propose an Inequality Constrained Shrinkage Estimator (ICSE) which takes the form of a weighted average between the unconstrained and inequality constrained estimators with the data dependent weight. The weight drives both the direction and degree of shrinkage. We use a local asymptotic framework to derive the asymptotic distribution and risk of the ICSE. We provide conditions under which the asymptotic risk of the ICSE is strictly less than that of the unrestricted extremum estimator. The degree of shrinkage cannot be consistently estimated under the local asymptotic framework. To address this issue, we propose a feasible plug-in estimator and investigate its finite sample behavior. We also apply our framework to gasoline demand estimation under the Slutsky restriction.},
  arxiv={2001.10586},
  abbr={arXiv},
  year={2020}
}

@misc{bakhitov2021dci,
 title={Deep Causal Inequalities: Demand Estimation using Individual-Level Data},
 author={Bakhitov, Edvard, and Singh, Amandeep, and Zhang, Jiding},
 note={working paper,},
 abstract={Modern machine learning algorithms can easily deal with unstructured data, however, recent literature has demonstrated that they do not perform well in presence of endogeneity in the explanatory variables. On the other hand, extant methods catered towards addressing endogeneity issues make strong parametric assumptions and, hence, are incapable of directly incorporating high-dimensional unstructured data. In this paper, we borrow from the literature on partial identification and propose the Deep Causal Inequalities (DeepCI) estimator that overcomes both these issues. Instead of relying on observed labels, the DeepCI estimator uses inferred moment inequalities from the observed behavior of agents in the data. This allows us to take care of endogeneity by differencing out unobservable product characteristics. We provide theoretical guarantees for our estimator and prove its consistency under very mild conditions. We demonstrate through extensive Monte Carlo simulations that our estimator outperforms standard supervised machine learning algorithms and existing partial identification methods. Finally, we apply DeepCI to the differentiated products demand estimation framework. The flexibility of the method allows for highly unstructured data like images, which we exploit in the empirical application based on the consumer-level car rental data from Hertz. Using the DeepCI estimator, we show how to estimate the importance of various car design features affecting consumer rental decisions.},
 year={2021}
}

@misc{bakhitov2022admle,
title={Automatic Debiased Machine Learning in Presence of Endogeneity (Job Market Paper)},
author={Edvard Bakhitov},
note={working paper,},
abstract={Recent advances in machine learning literature provide a series of new algorithms that both address endogeneity and can be applied in high-dimensional environments, we call them MLIV. This paper introduces an approach for performing valid asymptotic inference on regular functionals of MLIV estimators. The approach is based on construction of an orthogonal moment function that has a zero derivative with respect to the MLIV estimator. The debiasing is automatic in the sense that it only depends on the form of the identifying moment function but not on the form of the bias correction term. We derive a convergence rate for the penalized GMM estimator of the bias correction term. We also give conditions for root-n consistency and asymptotic normality of the debiased MLIV estimator of the functional of interest. Overall, the approach allows for a large variety of MLIV estimators as long as they satisfy mild convergence rate conditions. We apply our procedure to estimate the conditional demand derivative within the nonparametric demand for differentiated goods framework. Using both simulated and real data, we demonstrate that our debiased estimates have significantly reduced bias and close to the nominal level coverage, while the plug-in estimates perform poorly.},
year={2022},
pdf={jmp_edbakhitov.pdf}
}

@inproceedings{bakhitov2022causal,
  title={Causal Gradient Boosting: Boosted Instrumental Variable Regression},
  author={Edvard Bakhitov and Amandeep Singh},
  arxiv={2101.06078},
  abstract={Recent advances in the literature have demonstrated that standard supervised learning algorithms are ill-suited for problems with endogenous explanatory variables. To correct for the endogeneity bias, many variants of nonparameteric instrumental variable regression methods have been developed. In this paper, we propose an alternative algorithm called boostIV that builds on the traditional gradient boosting algorithm and corrects for the endogeneity bias. The algorithm is very intuitive and resembles an iterative version of the standard 2SLS estimator. Moreover, our approach is data driven, meaning that the researcher does not have to make a stance on neither the form of the target function approximation nor the choice of instruments. We demonstrate that our estimator is consistent under mild conditions. We carry out extensive Monte Carlo simulations to demonstrate the finite sample performance of our algorithm compared to other recently developed methods. We show that boostIV is at worst on par with the existing methods and on average significantly outperforms them.},
  code={https://github.com/edbakhitov/boostIV},
  abbr={EC 2022},
  isbn={9781450391504},
  publisher={Association for Computing Machinery},
  url={https://doi.org/10.1145/3490486.3538251},
  doi={10.1145/3490486.3538251},
  year={2022}
}

@misc{shi2023randomization,
 title={Randomization Inference for Bipartite Experiments},
 author={Shi, Liang, and Bakhitov, Edvard, and Hung, Kenneth},
 note={conference paper,},
 year={2023},
 abbr={CODE@MIT},
 url={https://drive.google.com/file/d/1sAQLMQEzVuD6ZfqfglIFu0a0GpmvRr59/view}
}

@misc{shi2024scalable,
 title={Scalable Analysis of Bipartite Experiments},
 author={Shi, Liang, and Bakhitov, Edvard, and Hung, Kenneth, and Karrer, Brian, and Walker, Charlie, and Bhole, Monica, and Schrijvers, Okke},
 note={working paper,},
 abstract={Bipartite Experiments are randomized experiments where the treatment is applied to a set of units (randomization units) that is different from the units of analysis, and randomization units and analysis units are connected through a bipartite graph. The scale of experimentation at large online platforms necessitates both accurate inference in the presence of a large bipartite interference graph, as well as a highly scalable implementation. In this paper, we describe new methods for inference that enable practical, scalable analysis of bipartite experiments: (1) We propose CA-ERL, a covariate-adjusted variant of the exposure-reweighted-linear (ERL) estimator [9], which empirically yields 60-90% variance reduction. (2) We introduce a randomization-based method for inference and prove asymptotic validity of a Wald-type confidence interval under graph sparsity assumptions. (3) We present a linear-time algorithm for randomization inference of the CA-ERL estimator, which can be easily implemented in query engines like Presto or Spark. We evaluate our methods both on a real experiment at Meta that randomized treatment on Facebook Groups and analyzed user-level metrics, as well as simulations on synthetic data. The real-world data shows that our CA-ERL estimator reduces the confidence interval (CI) width by 60-90% (compared to ERL) in a practical setting. The simulations using synthetic data show that our randomization inference procedure achieves correct coverage across instances, while the ERL estimator has incorrectly small CI widths for instances with large true effect sizes and is overly conservative when the bipartite graph is dense.},
 year={2024},
 arxiv={2402.11070},
 abbr={arXiv}
}

@misc{bakhitov2024acurt,
 title={Ad Clustered User Randomized Trials},
 author={Bakhitov, Edvard, and Zhang, Congshan, and Sherstobitov, Ivan, and Daley, Brett, and Ting, Daniel, and Nassif, Housam, and Leonenkov, Sergei},
 note={conference paper,},
 year={2024},
 abbr={CODE@MIT},
 url={https://drive.google.com/file/d/1W099j2g4apPL_9WLDik6Wi4273IE_fmg/view}
}

@misc{wang2024endbipartite,
 title={Experimentation on Endogenous Biaprtite Graphs},
 author={Wang, Wenshuo, and Bakhitov, Edvard, and Coey, Dominic, and Schrijvers, Okke},
 note={conference paper,},
 year={2024},
 abbr={CODE@MIT},
 url={https://drive.google.com/file/d/1NTPQgld8o11QvoD1VUfRbVf8nf2HFa8R/view}
}

@misc{wang2024endgraphs,
 title={Experimentation on Endogenous Graphs},
 author={Wang, Wenshuo, and Bakhitov, Edvard, and Coey, Dominic},
 note={working paper,},
 abstract={We study experimentation under endogenous network interference. Interference patterns are mediated by an endogenous graph, where edges can be formed or eliminated as a result of treatment. We show that conventional estimators are biased in these circumstances, and present a class of unbiased, consistent and asymptotically normal estimators of total treatment effects in the presence of such interference. Our results apply both to bipartite experimentation, in which the units of analysis and measurement differ, and the standard network experimentation case, in which they are the same.},
 year={2024},
 arxiv={2410.09267},
 abbr={arXiv}
}

@misc{bakhitov2025mliv,
 title={On Machine Learning Instrumental Variable Estimators},
 author={Bakhitov, Edvard},
 journal={SSRN working paper,},
 abstract={This paper examines the practical challenges arising from the ill-posedness of the nonparametric instrumental variable (NPIV) estimation problem. We show that conventional series NPIV estimators struggle to estimate the underlying structural function with desired precision even in ``moderate" dimensions. We argue that machine learning instrumental variable algorithms leverage sophisticated regularization techniques to mitigate these issues, achieving superior finite-sample performance.},
 year={2025},
 abbr={SSRN},
 url={https://ssrn.com/abstract=5258814}
}




