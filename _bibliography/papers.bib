---
---

@article{bakhitov2020frequentist,
  title={Frequentist Shrinkage under Inequality Constraints},
  author={Edvard Bakhitov},
  journal={working paper,},
  abstract={This paper shows how to shrink extremum estimators towards inequality constraints motivated by economic theory. We propose an Inequality Constrained Shrinkage Estimator (ICSE) which takes the form of a weighted average between the unconstrained and inequality constrained estimators with the data dependent weight. The weight drives both the direction and degree of shrinkage. We use a local asymptotic framework to derive the asymptotic distribution and risk of the ICSE. We provide conditions under which the asymptotic risk of the ICSE is strictly less than that of the unrestricted extremum estimator. The degree of shrinkage cannot be consistently estimated under the local asymptotic framework. To address this issue, we propose a feasible plug-in estimator and investigate its finite sample behavior. We also apply our framework to gasoline demand estimation under the Slutsky restriction.},
  arxiv={2001.10586},
  abbr={arXiv},
  year={2020}
}

@article{bakhitov2021admle,
title={Automatic Debiased Machine Learning in Presence of Endogeneity (Job Market Paper)},
author={Edvard Bakhitov},
journal={working paper,},
abstract={Recent advances in machine learning literature provide a series of new algorithms that both address endogeneity and can be applied to high-dimensional environments, we call them MLIV. This paper introduces an approach for performing valid asymptotic inference on regular functionals of MLIV estimators. The approach is based on construction of an orthogonal moment function that has a zero derivative with respect to the MLIV estimator. The debiasing is automatic in the sense that it only depends on the form of the identifying moment function but not on the form of the bias correction term. We derive a convergence rate for the penalized GMM estimator of the bias correction term. We also give conditions for root-n consistency and asymptotic normality of the debiased MLIV estimator of the functional of interest. Overall, the approach allows for a large variety of MLIV estimators as long as they satisfy mild convergence rate conditions. We apply our procedure to estimate the conditional demand derivative within the nonparametric demand for differentiated goods framework. Using both simulated and real data, we demonstrate that our debiased estimates have significantly reduced bias and close to the nominal level coverage, while the plug-in estimates perform poorly.},
year={2021}
}

@article{bakhitov2021causal,
  title={Causal Gradient Boosting: Boosted Instrumental Variable Regression},
  author={Edvard Bakhitov and Amandeep Singh},
  journal={working paper,},
  arxiv={2101.06078},
  abstract={Recent advances in the literature have demonstrated that standard supervised learning algorithms are ill-suited for problems with endogenous explanatory variables. To correct for the endogeneity bias, many variants of nonparameteric instrumental variable regression methods have been developed. In this paper, we propose an alternative algorithm called boostIV that builds on the traditional gradient boosting algorithm and corrects for the endogeneity bias. The algorithm is very intuitive and resembles an iterative version of the standard 2SLS estimator. Moreover, our approach is data driven, meaning that the researcher does not have to make a stance on neither the form of the target function approximation nor the choice of instruments. We demonstrate that our estimator is consistent under mild conditions. We carry out extensive Monte Carlo simulations to demonstrate the finite sample performance of our algorithm compared to other recently developed methods. We show that boostIV is at worst on par with the existing methods and on average significantly outperforms them.},
  code={https://github.com/edbakhitov/boostIV},
  abbr={arXiv},
  year={2021}
}

@article{bakhitov2021dci,
 title={Deep Causal Inequalities: Demand Estimation using Individual-Level Data},
 author={Bakhitov, Edvard, and Singh, Amandeep, and Zhang, Jiding},
 journal={working paper,},
 abstract={Modern machine learning algorithms can easily deal with unstructured data, however, recent literature has demonstrated that they do not perform well in presence of endogeneity in the explanatory variables. On the other hand, extant methods catered towards addressing endogeneity issues make strong parametric assumptions and, hence, are incapable of directly incorporating high-dimensional unstructured data. In this paper, we borrow from the literature on partial identification and propose the Deep Causal Inequalities (DeepCI) estimator that overcomes both these issues. Instead of relying on observed labels, the DeepCI estimator uses inferred moment inequalities from the observed behavior of agents in the data. This allows us to take care of endogeneity by differencing out unobservable product characteristics. We provide theoretical guarantees for our estimator and prove its consistency under very mild conditions. We demonstrate through extensive Monte Carlo simulations that our estimator outperforms standard supervised machine learning algorithms and existing partial identification methods. Finally, we apply DeepCI to the differentiated products demand estimation framework. The flexibility of the method allows for highly unstructured data like images, which we exploit in the empirical application based on the consumer-level car rental data from Hertz. Using the DeepCI estimator, we show how to estimate the importance of various car design features affecting consumer rental decisions.},
 year={2021}
}





